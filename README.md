# Demo AWS Data Pipeline
This Repo is assigned to manage codes used to complete this pipeline
Boto3 and Pyspark was used to complete pipeline

All codes were deployed locally using Jupyter Notebook. See ipynb file uploaded to repo.

Boto3 was used to connect to the public s3 bucket.

All relevant files were downloaded to a local folder.

PySpark Dataframes were created using the local files and loaded to the staging schema.

All transformations were deployed as SQL queries. PySpark Dataframes were created using the queries and loaded to the analytics schema.

Files were also exported to a local staging folder for export to AWS S3


